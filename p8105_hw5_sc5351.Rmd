---
title: "p8105_hw5_sc5351"
author: "Shaoyu Chen"
date: "2023-11-14"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 10, 
  fig.height = 8,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

#Describe the raw data. 
```{r}
homicide_df = 
  read_csv("./data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```
#In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 
```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```
#For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.
```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```
#Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 
```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

#Create a plot that shows the estimates and CIs for each city â€“ check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Problem 2

#a dataframe containing all file names
```{r}
longitudinal_df =
  tibble(
    file_names = list.files("./data/longitudinal_study/"),
    file_path = str_c("./data/longitudinal_study/",file_names),
    data = map(file_path,read_csv)
  )
```
#Iterate over file names and read in data for each subject and tidy the result
```{r}
tidy_df = 
  longitudinal_df|>
  unnest()|>
  janitor::clean_names() |>
  separate(file_names, into = c("arm", "subject_id"), sep = "_") |> 
  mutate(
    arm = case_match(
      arm, 
      "con" ~ "control", 
      "exp" ~ "experiment")) |>
   mutate(subject_id = str_replace(subject_id, ".csv", "")) |>
  pivot_longer(
    week_1: week_8,
    names_to = "week",
    values_to = "observation"
  ) |>
  select(arm, subject_id, week, observation)
```
#Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.
```{r}
tidy_df |>
  ggplot(aes(x = week, y = observation, group = subject_id, color = subject_id)) +
  geom_point()+
  geom_line() +
  labs(x = "week",
       y = "observation",
       title = "Spaghetti Plot of Observations Over Time") +
   theme_minimal()+
   facet_grid(~arm) 
```
# comment on differences between groups.
Based on this two plots, the experimental arm has higher value than control arm.

## Problem 3

#First set the following design elements
```{r}
n <- 30
sigma <- 5
mu <- 0:6
n_dataset <- 5000
alpha <- 0.05
```

#Perform t-test, return estimate, p-value and output 5000 list 
```{r}
set.seed(1)

output = vector("list", 5000)

t_test = function(mu){
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data |> 
    t.test(conf.level = 0.95) |> 
    broom::tidy() |> 
    select(estimate, p.value) 
}
```

#for mu = 0, simulation
```{r}
for (i in 1:5000) {
  output[[i]] = t_test(mu = 0)
}

mu_0 = bind_rows(output)
```

#for mu = 1,2,3,4,5,6, simulation

```{r}
sim_results_df = 
  expand_grid(
    mu = 1:6,
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(mu, t_test)
  ) |> 
  unnest(estimate_df)
```